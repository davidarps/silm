{
    "r2d2": {
        "architectures": [
          "Bert"
        ],
        "model_type": "bert",
        "attention_probs_dropout_prob": 0.1,
        "hidden_act": "gelu",
        "hidden_dropout_prob": 0.1,
        "hidden_size": 128,
        "const_size": 128,
        "initializer_range": 0.02,
        "intermediate_size": 96,
        "score_dim": 128,
        "max_role_embeddings": 4,
        "num_attention_heads": 4,
        "type_vocab_size": 2,
        "vocab_size": 133,
        "encoder_num_hidden_layers": 3,
        "decoder_num_hidden_layers": 1,
        "pairwise_label_num": 2,
        "pad_token_id": 1,
        "bos_token_id": 101,
        "eos_token_id": 102,
        "reduce_token_id": 50257,
        "cls_token_id": 101,
        "sum_token_id": 7,
        "mask_token_id": 103,
        "nsp_token_id": 8,
        "lr_token_id": 9,
        "rr_token_id": 10,
        "eot_token_id": 11,
        "tree_mask_token_id": 12,
        "policy_token_id": 6,
        "window_size": 2,
        "enable_self_attention": true,
        "parser_input_dim": 128,
        "parser_hidden_dim": 96,
        "parser_num_layers": 3,
        "parser_max_len":192,
        "parser_nhead":4,
        "parser_chunked": true,
        "ext_vocab_size": -1
    },
    "gpt": {
        "activation_function": "gelu_new",
        "architectures": [
          "GPT2LMHeadModel"
        ],
        "attn_pdrop": 0.1,
        "dense_hidden_factor": 1,
        "bos_token_id": 0,
        "embd_pdrop": 0.1,
        "eos_token_id": 2,
        "initializer_range": 0.02,
        "layer_norm_epsilon": 1e-05,
        "model_type": "gpt2",
        "hidden_size": 96,
        "max_position_embeddings": 192,
        "n_ctx": 192,
        "n_embd": 128,
        "n_head": 4,
        "n_inner": 96,
        "n_layer": 6,
        "n_positions": 192,
        "resid_pdrop": 0.1,
        "summary_activation": null,
        "summary_first_dropout": 0.1,
        "summary_proj_to_labels": true,
        "summary_type": "cls_index",
        "summary_use_proj": true,
        "task_specific_params": {
          "text-generation": {
            "do_sample": true,
            "max_length": 50
          }
        },
        "vocab_size": 133,
        "output_hidden_states": "True",
        "action_layer_num": 3
    }
}