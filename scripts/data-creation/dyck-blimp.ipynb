{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e5dd39",
   "metadata": {},
   "source": [
    "# BLIMP-style benchmark for the Dyck and Dyck-u languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2290a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk import Tree\n",
    "dataset = \"data/dyckkm/dyckkm_k64_m7_100000000.dev.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d17bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import CFG, ChartParser\n",
    "\n",
    "def make_dycku_grammar():\n",
    "\n",
    "    rules_basic = []\n",
    "    rules_basic.append(\"S -> N V\")\n",
    "    rules_basic.append(\"S -> NSg V\")\n",
    "    rules_basic.append(\"S -> NSg VSg\")\n",
    "    rules_basic.append(\"S -> NPl V\")\n",
    "    rules_basic.append(\"S -> NPl VPl\")\n",
    "    rules_basic.append(\"S -> N VSg\")\n",
    "    rules_basic.append(\"S -> N VPl\")\n",
    "    rules_basic.append(\"S -> N S V\")\n",
    "    rules_basic.append(\"S -> NSg S VSg\")\n",
    "    rules_basic.append(\"S -> NPl S VPl\")\n",
    "    rules_basic.append(\"S -> NSg S V\")\n",
    "    rules_basic.append(\"S -> NPl S V\")\n",
    "    rules_basic.append(\"S -> N S VSg\")\n",
    "    rules_basic.append(\"S -> N S VPl\")\n",
    "    rules_basic.append(\"N -> 'n'\")\n",
    "    rules_basic.append(\"NSg -> 'nsg'\")\n",
    "    rules_basic.append(\"NPl -> 'npl'\")\n",
    "    rules_basic.append(\"V -> 'v'\")\n",
    "    rules_basic.append(\"VSg -> 'vsg'\")\n",
    "    rules_basic.append(\"VPl -> 'vpl'\")\n",
    "\n",
    "    rules_conj = [r + \" S\" for r in rules_basic if r.startswith(\"S\")] # version with many rules but no structural ambiguity\n",
    "    #rules_conj = [\"S -> S S\"] # less rules but many parses\n",
    "\n",
    "    rules = rules_basic + rules_conj\n",
    "    grammar = CFG.fromstring(\"\\n\".join(rules))\n",
    "    parser = ChartParser(grammar)\n",
    "    gr = parser.grammar() # grammar but slightly different. \n",
    "    return gr, parser\n",
    "\n",
    "import string\n",
    "def get_identifier_iterator():\n",
    "  \"\"\" Returns an iterator to provide unique ids to bracket types.\n",
    "  \"\"\"\n",
    "  ids = iter(list(string.ascii_lowercase))\n",
    "  k = 1\n",
    "  while True:\n",
    "    try:\n",
    "      str_id = next(ids)\n",
    "    except StopIteration:\n",
    "      ids = iter(list(string.ascii_lowercase))\n",
    "      k += 1\n",
    "      str_id = next(ids)\n",
    "    yield str_id*k\n",
    "\n",
    "def get_vocab_of_bracket_types(bracket_types):\n",
    "  \"\"\" Returns the vocabulary corresponding to the number of brackets.\n",
    "\n",
    "  There are bracket_types open brackets, bracket_types close brackets,\n",
    "  START, and END.\n",
    "  Arguments:\n",
    "    bracket_types: int (k in Dyck-(k,m))\n",
    "  Returns:\n",
    "    Dictionary mapping symbol string  s to int ids.\n",
    "  \"\"\"\n",
    "  id_iterator = get_identifier_iterator()\n",
    "  ids = [next(id_iterator) for x in range(bracket_types)]\n",
    "  vocab = {x: c for c, x in enumerate(['(' + id_str for id_str in ids] + [id_str + ')' for id_str in ids] + ['START', 'END'])}\n",
    "  return vocab, ids\n",
    "\n",
    "def make_dyck1_grammar():\n",
    "    rules_basic = []\n",
    "    rules_basic.append(\"S -> '<a' 'a>'\")\n",
    "    rules_basic.append(\"S -> '<a' S 'a>'\")\n",
    "    #rules_basic.append(\"S -> '<a' 'a>' S\")\n",
    "    rules_conj = [r + \" S\" for r in rules_basic if r.startswith(\"S\")] # version with many rules but no structural ambiguity\n",
    "    #rules_conj = [\"S -> S S\"] # less rules but many parses\n",
    "\n",
    "    rules = rules_basic + rules_conj\n",
    "    grammar = CFG.fromstring(\"\\n\".join(rules))\n",
    "    parser = ChartParser(grammar)\n",
    "    gr = parser.grammar() # grammar but slightly different. \n",
    "    return gr, parser\n",
    "_, dyck1_parser = make_dyck1_grammar()\n",
    "def make_dyck2_grammar():\n",
    "    rules_basic = []\n",
    "    lits = [\"a\", \"b\"]\n",
    "    for lit in lits:\n",
    "        rules_basic.append(f\"S -> '<{lit}' '{lit}>'\")\n",
    "        rules_basic.append(f\"S -> '<{lit}' S '{lit}>'\")\n",
    "    #rules_basic.append(\"S -> '<a' 'a>' S\")\n",
    "    rules_conj = [r + \" S\" for r in rules_basic if r.startswith(\"S\")] # version with many rules but no structural ambiguity\n",
    "    #rules_conj = [\"S -> S S\"] # less rules but many parses\n",
    "\n",
    "    rules = rules_basic + rules_conj\n",
    "    grammar = CFG.fromstring(\"\\n\".join(rules))\n",
    "    parser = ChartParser(grammar)\n",
    "    gr = parser.grammar() # grammar but slightly different. \n",
    "    return gr, parser\n",
    "\n",
    "def make_dyck64_grammar():\n",
    "    rules_basic = []\n",
    "    for lit in get_vocab_of_bracket_types(64)[1]:\n",
    "        rules_basic.append(f\"S -> '<{lit}' '{lit}>'\")\n",
    "        rules_basic.append(f\"S -> '<{lit}' S '{lit}>'\")\n",
    "    #rules_basic.append(\"S -> '<a' 'a>' S\")\n",
    "    rules_conj = [r + \" S\" for r in rules_basic if r.startswith(\"S\")] # version with many rules but no structural ambiguity\n",
    "    #rules_conj = [\"S -> S S\"] # less rules but many parses\n",
    "\n",
    "    rules = rules_basic + rules_conj\n",
    "    grammar = CFG.fromstring(\"\\n\".join(rules))\n",
    "    parser = ChartParser(grammar)\n",
    "    gr = parser.grammar() # grammar but slightly different. \n",
    "    return gr, parser\n",
    "\n",
    "def make_dyck_parser(k):\n",
    "    if k==1:\n",
    "       return make_dyck1_grammar()[1]\n",
    "    if k==2:\n",
    "       return make_dyck2_grammar()[1]\n",
    "    if k==64:\n",
    "       return make_dyck64_grammar()[1]\n",
    "_, dyck1_parser = make_dyck1_grammar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecaa97e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from silm.gpst.eval_utils import load_gold_data\n",
    "import tqdm\n",
    "\n",
    "span_to_create = 2\n",
    "\n",
    "def create_blimp_data_for_file(dataset, span_to_create, parser):\n",
    "    to_skip = []\n",
    "    texts, trees = load_gold_data(dataset, bos_tok=\"\", eos_tok=\"\")\n",
    "    for sent_ix,(tree, tokens) in tqdm.tqdm(enumerate(zip(trees, texts))):\n",
    "        tokens = tokens.split()\n",
    "        assert tree.leaves() == tokens\n",
    "        success = False\n",
    "        ixs_to_try = list(range(len(tree.leaves())))\n",
    "        random.shuffle(ixs_to_try)\n",
    "        for chosen_index in ixs_to_try:        \n",
    "            #chosen_index = random.choice(range(len(tree.leaves())))\n",
    "            chosen_token = tokens[chosen_index]\n",
    "\n",
    "            leaf_tp = tree.leaf_treeposition(chosen_index)\n",
    "            if leaf_tp[-1] == 0:\n",
    "                subtrees = [tree] # relevant subtree\n",
    "                for i in leaf_tp[:-1]:\n",
    "                    subtrees.append(subtrees[-1][i])\n",
    "                relevant_subtree = subtrees[-1]\n",
    "                span = len(relevant_subtree.leaves())\n",
    "                closing_token = f\"{chosen_token[1:]}>\"\n",
    "                if span==2 == span_to_create == 2: # ( ) case\n",
    "                    relevant_subtree[0] = closing_token\n",
    "                    relevant_subtree[1] = chosen_token\n",
    "                    #for subtree, i in zip(subtrees[-2::-1], leaf_tp[-2::-1]):\n",
    "                    #    subtree[i] = relevant_subtree\n",
    "                    #    relevant_subtree = subtree[i]\n",
    "                    if len(list(parser.parse(tree.leaves()))) == 0:\n",
    "                        success = True\n",
    "                    \n",
    "                if span==span_to_create: # ( ( ) ) case\n",
    "                    relevant_subtree[0] = closing_token\n",
    "                    relevant_subtree[-1] = chosen_token\n",
    "                    #for subtree, i in zip(subtrees[-2::-1], leaf_tp[-2::-1]):\n",
    "                    #    subtree[i] = relevant_subtree\n",
    "                    #    relevant_subtree = subtree[i]\n",
    "                    if len(list(parser.parse(tree.leaves()))) == 0:\n",
    "                        success = True\n",
    "                    \n",
    "                if success:\n",
    "                    break\n",
    "        if not success:\n",
    "            to_skip.append(sent_ix)\n",
    "    good, bad = [], []\n",
    "    for sent_ix, (tree, text) in enumerate(zip(trees, texts)):\n",
    "        if sent_ix not in to_skip:\n",
    "            good.append(text)\n",
    "            bad.append(\" \".join(tree.leaves()))\n",
    "    return good, bad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a20aa8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3925it [00:00, 4685.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 12 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3925it [00:02, 1424.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2798it [00:01, 2165.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 12 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2798it [00:04, 588.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3895it [00:00, 4514.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 12 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3895it [00:02, 1658.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2851it [00:01, 2228.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 12 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2851it [00:03, 812.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 3 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3946it [00:00, 4642.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 12 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3946it [00:01, 1997.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 3 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2750it [00:01, 2053.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 12 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2750it [00:02, 941.12it/s] \n"
     ]
    }
   ],
   "source": [
    "ks = [1,2,64]\n",
    "splits = [\"dev\",\"test\"]\n",
    "spans = [3,12] #[1,48,64]# [16,24,32] # [2,4,6,8,10]\n",
    "datasets = []\n",
    "\n",
    "import json, random\n",
    "blimp_datasets = []\n",
    "for k in ks:\n",
    "    parser = make_dyck_parser(k)\n",
    "    for split in splits:\n",
    "        dataset = f\"data/dyckkm/dyckkm_k{k}_m7_100000000.{split}.json\"\n",
    "        \n",
    "        for span in spans: \n",
    "            print(k, span, split)\n",
    "            good, bad = create_blimp_data_for_file(dataset, span, parser)\n",
    "            with open(f\"data/blimpfordyck/dyck_k{k}/dyck_k{k}_{split}_blimpdyck_{span}.jsonl\", \"w\") as f:\n",
    "                for g,b in zip(good, bad):\n",
    "                    f.write(json.dumps({\n",
    "                        \"sentence_good\": g,\n",
    "                        \"sentence_bad\": b\n",
    "                    }))\n",
    "                    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ebe4781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4132it [00:02, 1670.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4132it [00:05, 721.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2485it [00:04, 518.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2485it [00:10, 245.68it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "# same for Dyck-u\n",
    "splits = [\"dev\",\"test\"]\n",
    "spans = [3,12]# [1,48,64]# [16,24,32] # [2,4,6,8,10]\n",
    "datasets = []\n",
    "\n",
    "from silm.gpst.eval_utils import load_gold_data\n",
    "import tqdm\n",
    "\n",
    "def create_blimp_data_for_dycku_file(dataset, span_to_create):\n",
    "    to_skip = []\n",
    "    parser = make_dycku_grammar()[1]\n",
    "    texts, trees = load_gold_data(dataset, bos_tok=\"\", eos_tok=\"\")\n",
    "    for sent_ix,(tree, tokens) in tqdm.tqdm(enumerate(zip(trees, texts))):\n",
    "        tokens = tokens.split()\n",
    "        assert tree.leaves() == tokens\n",
    "        success=False\n",
    "        ixs_to_try = list(range(len(tree.leaves())))\n",
    "        random.shuffle(ixs_to_try)\n",
    "        for chosen_index in ixs_to_try: \n",
    "            chosen_index = random.choice(range(len(tree.leaves())))\n",
    "            chosen_token = tokens[chosen_index]\n",
    "\n",
    "            leaf_tp = tree.leaf_treeposition(chosen_index)\n",
    "            if leaf_tp[-1] == 0:\n",
    "                subtrees = [tree] # relevant subtree\n",
    "                for i in leaf_tp[:-2]:\n",
    "                    subtrees.append(subtrees[-1][i])\n",
    "                relevant_subtree = subtrees[-1]\n",
    "                span = len(relevant_subtree.leaves())\n",
    "                closing_token = f\"{chosen_token[1:]}>\"\n",
    "                if span==2 == span_to_create == 2: # ( ) case\n",
    "                    #relevant_subtree[0] = closing_token\n",
    "                    #relevant_subtree[1] = chosen_token\n",
    "                    opening_subtree = relevant_subtree[0]\n",
    "                    closing_subtree = relevant_subtree[1]\n",
    "                    relevant_subtree[0] = closing_subtree\n",
    "                    relevant_subtree[1] = opening_subtree\n",
    "                    #for subtree, i in zip(subtrees[-2::-1], leaf_tp[-2::-1]):\n",
    "                    #    subtree[i] = relevant_subtree\n",
    "                    #    relevant_subtree = subtree[i]\n",
    "                    if len(list(parser.parse(tree.leaves()))) == 0:\n",
    "                        success = True\n",
    "                    #break\n",
    "                elif span==span_to_create: # ( ( ) ) case\n",
    "                    opening_subtree = relevant_subtree[0]\n",
    "                    if len(relevant_subtree[-1].leaves())==1: \n",
    "                        closing_subtree = relevant_subtree[-1]\n",
    "                        relevant_subtree[0] = closing_subtree\n",
    "                        relevant_subtree[-1] = opening_subtree\n",
    "                        #for subtree, i in zip(subtrees[-2::-1], leaf_tp[-2::-1]):\n",
    "                        #    subtree[i] = relevant_subtree\n",
    "                        #    relevant_subtree = subtree[i]\n",
    "                        if len(list(parser.parse(tree.leaves()))) == 0:\n",
    "                            success = True\n",
    "                        #break\n",
    "                if success:\n",
    "                    break\n",
    "                \n",
    "        if not success:\n",
    "            to_skip.append(sent_ix)\n",
    "    good, bad = [], []\n",
    "    for sent_ix, (tree, text) in enumerate(zip(trees, texts)):\n",
    "        if sent_ix not in to_skip:\n",
    "            good.append(text)\n",
    "            bad.append(\" \".join(tree.leaves()))\n",
    "    return good, bad \n",
    "\n",
    "import json\n",
    "blimp_datasets = []\n",
    "for split in splits:\n",
    "    dataset = f\"data/dyckkm/underspec_100000000.{split}.json\"\n",
    "    \n",
    "    for span in spans: \n",
    "        print(span, split)\n",
    "        good, bad = create_blimp_data_for_dycku_file(dataset, span)\n",
    "        with open(f\"data/blimpfordyck/dyck_u/dyck_u_{split}_blimpdyck_{span}.jsonl\", \"w\") as f:\n",
    "            for g,b in zip(good, bad):\n",
    "                f.write(json.dumps({\n",
    "                    \"sentence_good\": g,\n",
    "                    \"sentence_bad\": b\n",
    "                }))\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e771d23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1244, 4132)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(good), 4132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d768cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('n n npl v nsg n vsg nsg nsg v v n vpl n v nsg nsg npl npl npl nsg v vpl v v npl vpl vsg npl nsg n npl vpl v vsg v v vsg vpl npl n v vpl npl nsg npl vpl vsg vpl vsg n nsg v vsg',\n",
       " 'n n npl v nsg n vsg nsg nsg v v n vpl n v nsg nsg npl npl npl nsg v vpl v v npl vpl vsg v nsg n npl vpl v vsg npl v vsg vpl npl n v vpl npl nsg npl vpl vsg vpl vsg n nsg v vsg')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good[1], bad[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47cd0ad",
   "metadata": {},
   "source": [
    "## Other data types\n",
    "\n",
    "- `type_mismatch` Replacing brackets with brackets from other bracket types (different numbers of brackets, closing vs. opening)\n",
    "- `break_bracketing` Swapping randomly chosen brackets (with different linear distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957b9860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_mismatch: \n",
    "dataset = 'data/dyckkm/dyckkm_k64_m7_100000000.dev.json'\n",
    "texts, trees = load_gold_data(dataset, bos_tok=\"\", eos_tok=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b16cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = make_dyck_parser(64)\n",
    "vocabulary = get_vocab_of_bracket_types(64)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f11ce784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_type_mismatch_data(num_replacements, texts, parser, vocabulary):\n",
    "    results = []\n",
    "    for sent in tqdm.tqdm(texts):\n",
    "        success = False\n",
    "        if len(sent.split()) < num_replacements:\n",
    "            continue\n",
    "        while not success:\n",
    "            replaced_ixs = set()\n",
    "            sentsplit = sent.split()\n",
    "            for i in range(num_replacements):\n",
    "                ix_to_replace = random.choice(list(set(range(len(sentsplit))) - replaced_ixs))\n",
    "                old_token = sentsplit[ix_to_replace]\n",
    "                opening = old_token.startswith(\"<\")\n",
    "                if opening:\n",
    "                    new_token = f\"<{random.choice(vocabulary)}\"\n",
    "                else:\n",
    "                    new_token = f\"{random.choice(vocabulary)}>\"\n",
    "\n",
    "                sentsplit[ix_to_replace] = new_token\n",
    "                replaced_ixs.add(ix_to_replace)\n",
    "\n",
    "            if len(list(parser.parse(sentsplit))) == 0:\n",
    "                success = True\n",
    "                results.append((sent, ' '.join(sentsplit)))\n",
    "    return [res[0] for res in results], [res[1] for res in results]\n",
    "\n",
    "def create_type_mismatch_u_data(num_replacements, texts):\n",
    "    \n",
    "    parser = make_dycku_grammar()[1]\n",
    "    results = []\n",
    "\n",
    "    max_tries = 100\n",
    "    for sent in tqdm.tqdm(texts):\n",
    "        success = False\n",
    "        num_tries = 0\n",
    "        if len(sent.split()) < num_replacements:\n",
    "            continue\n",
    "        vocabulary = [\"\", \"sg\", \"pl\"]\n",
    "        while not success:\n",
    "            replaced_ixs = set()\n",
    "            sentsplit = sent.split()\n",
    "            for i in range(num_replacements):\n",
    "                ix_to_replace = random.choice(list(set(range(len(sentsplit))) - replaced_ixs))\n",
    "                old_token = sentsplit[ix_to_replace]\n",
    "                opening = old_token.startswith(\"n\")\n",
    "                if opening:\n",
    "                    new_token = f\"n{random.choice(vocabulary)}\"\n",
    "                else:\n",
    "                    new_token = f\"v{random.choice(vocabulary)}\"\n",
    "\n",
    "                sentsplit[ix_to_replace] = new_token\n",
    "                replaced_ixs.add(ix_to_replace)\n",
    "\n",
    "            if len(list(parser.parse(sentsplit))) == 0:\n",
    "                success = True\n",
    "                results.append((sent, ' '.join(sentsplit)))\n",
    "            if not success:\n",
    "                num_tries += 1\n",
    "                if num_tries >= max_tries:\n",
    "                    success=True \n",
    "    return [res[0] for res in results], [res[1] for res in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9b1e8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 24 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3895/3895 [00:01<00:00, 2152.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 32 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3895/3895 [00:01<00:00, 2535.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 48 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3895/3895 [00:01<00:00, 3674.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 64 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3895/3895 [00:00<00:00, 6816.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 24 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2851/2851 [00:02<00:00, 1262.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 32 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2851/2851 [00:02<00:00, 1384.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 48 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2851/2851 [00:01<00:00, 1689.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 64 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2851/2851 [00:01<00:00, 2068.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 24 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3946/3946 [00:01<00:00, 2735.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 32 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3946/3946 [00:01<00:00, 3211.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 48 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3946/3946 [00:00<00:00, 4878.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 64 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3946/3946 [00:00<00:00, 8455.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 24 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2750/2750 [00:01<00:00, 1408.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 32 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2750/2750 [00:01<00:00, 1535.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 48 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2750/2750 [00:01<00:00, 1897.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 64 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2750/2750 [00:01<00:00, 2389.66it/s]\n"
     ]
    }
   ],
   "source": [
    "ks = [2,64]\n",
    "splits = [\"dev\",\"test\"]\n",
    "num_replacements = [24,32,48,64] # [1,2,3,4,6,8,10,12,16]\n",
    "datasets = []\n",
    "\n",
    "import json, random\n",
    "blimp_datasets = []\n",
    "for k in ks:\n",
    "    parser = make_dyck_parser(k)\n",
    "    vocabulary = get_vocab_of_bracket_types(k)[1]\n",
    "    for split in splits:\n",
    "        dataset = f\"data/dyckkm/dyckkm_k{k}_m7_100000000.{split}.json\"\n",
    "        texts, _ = load_gold_data(dataset, bos_tok=\"\", eos_tok=\"\")\n",
    "        for num_repl in num_replacements: \n",
    "            print(k, num_repl, split)\n",
    "            \n",
    "            good, bad = create_type_mismatch_data(num_repl, texts, parser, vocabulary)\n",
    "            with open(f\"data/blimpfordyck/dyck_k{k}/dyck_k{k}_{split}_blimpdycktypemismatch_{num_repl}.jsonl\", \"w\") as f:\n",
    "                for g,b in zip(good, bad):\n",
    "                    f.write(json.dumps({\n",
    "                        \"sentence_good\": g,\n",
    "                        \"sentence_bad\": b\n",
    "                    }))\n",
    "                    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4d39573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4132/4132 [00:01<00:00, 2562.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2485/2485 [00:04<00:00, 568.35it/s]\n"
     ]
    }
   ],
   "source": [
    "splits = [\"dev\",\"test\"]\n",
    "num_replacements = [64]# [24,32,48,64]# [4,6,8,10,12,16] #[1,2,3]#,4,6,8,10,12,16]\n",
    "datasets = []\n",
    "\n",
    "import json, random\n",
    "\n",
    "for split in splits:\n",
    "    dataset = f\"data/dyckkm/underspec_100000000.{split}.json\"\n",
    "    texts, _ = load_gold_data(dataset, bos_tok=\"\", eos_tok=\"\")\n",
    "    for num_repl in num_replacements: \n",
    "        print(num_repl, split)\n",
    "        good, bad = create_type_mismatch_u_data(num_repl, texts)\n",
    "        with open(f\"data/blimpfordyck/dyck_u/dyck_u_{split}_blimpdycktypemismatch_{num_repl}.jsonl\", \"w\") as f:\n",
    "            for g,b in zip(good, bad):\n",
    "                f.write(json.dumps({\n",
    "                    \"sentence_good\": g,\n",
    "                    \"sentence_bad\": b\n",
    "                }))\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257eb235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# break bracketing\n",
    "def create_type_breakbracketing_data(distance, texts, parser):\n",
    "    results = []\n",
    "    for sent in tqdm.tqdm(texts):\n",
    "        sentsplit = sent.split()\n",
    "        if len(sentsplit) < distance + 2:\n",
    "            continue\n",
    "        success = False\n",
    "        max_tries = 20\n",
    "        num_tries = 0\n",
    "        while not success:\n",
    "            sentsplit = sent.split()\n",
    "            \n",
    "            ix_to_replace = random.choice(range(len(sentsplit) - distance))\n",
    "            old_token_first = sentsplit[ix_to_replace]\n",
    "            old_token_second = sentsplit[ix_to_replace+distance]\n",
    "\n",
    "            sentsplit[ix_to_replace] = old_token_second\n",
    "            sentsplit[ix_to_replace+distance] = old_token_first\n",
    "\n",
    "            if len(list(parser.parse(sentsplit))) == 0:\n",
    "                success = True\n",
    "                results.append((sent, ' '.join(sentsplit)))\n",
    "            if not success:\n",
    "                num_tries += 1 \n",
    "                if num_tries >= max_tries:\n",
    "                    success=True\n",
    "    return [res[0] for res in results], [res[1] for res in results]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f885fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 48 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3925/3925 [00:08<00:00, 462.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 64 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3925/3925 [00:03<00:00, 1141.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 48 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2798/2798 [00:19<00:00, 146.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 64 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2798/2798 [00:13<00:00, 214.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 48 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3895/3895 [00:02<00:00, 1692.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 64 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3895/3895 [00:01<00:00, 3024.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 48 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2851/2851 [00:03<00:00, 773.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 64 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2851/2851 [00:03<00:00, 949.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 48 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3946/3946 [00:01<00:00, 2805.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 64 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3946/3946 [00:00<00:00, 5064.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 48 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2750/2750 [00:02<00:00, 1092.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 64 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2750/2750 [00:02<00:00, 1298.39it/s]\n"
     ]
    }
   ],
   "source": [
    "ks = [1,2,64]\n",
    "splits = [\"dev\",\"test\"]\n",
    "distances = [48,64]# [1,2,3,4,6,8,10,12,16,24,32]\n",
    "datasets = []\n",
    "\n",
    "import json, random\n",
    "blimp_datasets = []\n",
    "for k in ks:\n",
    "    parser = make_dyck_parser(k)\n",
    "    vocabulary = get_vocab_of_bracket_types(k)[1]\n",
    "    for split in splits:\n",
    "        dataset = f\"data/dyckkm/dyckkm_k{k}_m7_100000000.{split}.json\"\n",
    "        texts, _ = load_gold_data(dataset, bos_tok=\"\", eos_tok=\"\")\n",
    "        for distance in distances: \n",
    "            print(k, distance, split)\n",
    "            \n",
    "            good, bad = create_type_breakbracketing_data(distance, texts, parser)\n",
    "            with open(f\"data/blimpfordyck/dyck_k{k}/dyck_k{k}_{split}_blimpdyckbrokenbracketing_{distance}.jsonl\", \"w\") as f:\n",
    "                for g,b in zip(good, bad):\n",
    "                    f.write(json.dumps({\n",
    "                        \"sentence_good\": g,\n",
    "                        \"sentence_bad\": b\n",
    "                    }))\n",
    "                    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "908235c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4132/4132 [00:30<00:00, 137.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4132/4132 [00:30<00:00, 135.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4132/4132 [00:21<00:00, 196.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4132/4132 [00:22<00:00, 186.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4132/4132 [00:19<00:00, 216.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4132/4132 [00:17<00:00, 237.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4132/4132 [00:15<00:00, 265.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4132/4132 [00:14<00:00, 278.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4132/4132 [00:12<00:00, 318.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4132/4132 [00:10<00:00, 382.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4132/4132 [00:08<00:00, 466.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4132/4132 [00:05<00:00, 766.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4132/4132 [00:04<00:00, 1019.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2485/2485 [00:34<00:00, 71.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2485/2485 [00:32<00:00, 75.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2485/2485 [00:23<00:00, 104.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2485/2485 [00:24<00:00, 101.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2485/2485 [00:21<00:00, 115.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2485/2485 [00:20<00:00, 120.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2485/2485 [00:18<00:00, 135.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2485/2485 [00:17<00:00, 145.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2485/2485 [00:16<00:00, 150.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2485/2485 [00:14<00:00, 168.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2485/2485 [00:13<00:00, 187.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2485/2485 [00:11<00:00, 215.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2485/2485 [00:11<00:00, 223.96it/s]\n"
     ]
    }
   ],
   "source": [
    "splits = [\"dev\", \"test\"]\n",
    "distances = [1,2,3,4,6,8,10,12,16,24,32,48,64]\n",
    "parser = make_dycku_grammar()[1]\n",
    "for split in splits:\n",
    "    dataset = f\"data/dyckkm/underspec_100000000.{split}.json\"\n",
    "    texts, _ = load_gold_data(dataset, bos_tok=\"\", eos_tok=\"\")\n",
    "    for distance in distances: \n",
    "        print(distance, split)\n",
    "        good, bad = create_type_breakbracketing_data(distance, texts, parser)\n",
    "        with open(f\"data/blimpfordyck/dyck_u/dyck_u_{split}_blimpdyckbrokenbracketing_{distance}.jsonl\", \"w\") as f:\n",
    "            for g,b in zip(good, bad):\n",
    "                f.write(json.dumps({\n",
    "                    \"sentence_good\": g,\n",
    "                    \"sentence_bad\": b\n",
    "                }))\n",
    "                f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".silm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
